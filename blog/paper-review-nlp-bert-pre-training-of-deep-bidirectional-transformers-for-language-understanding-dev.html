<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding  (Devlin et al., 2021) - Cheonkam Jeong</title>
    <link rel="stylesheet" href="../css/style.css">
</head>
<body>
    <article class="blog-post">
        <h1>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding  (Devlin et al., 2021)</h1>
        <p class="post-date">December 05, 2022</p>

        <p><span style="font-size: medium;"><b>&nbsp;<span style="caret-color: rgb(0, 0, 0);"><span style="font-family: helvetica;">Abstract</span></span></b></span></p><p><span style="caret-color: rgb(0, 0, 0);"><span style="font-family: helvetica;">Introducing BERT (Bidirectional Encoder Representations from Transformers); bidirectional - jointly conditioning on both left and right context in all layers, which creates state-of-art models for a wide range of tasks</span></span></p><p><span style="caret-color: rgb(0, 0, 0);"><span style="font-family: helvetica; font-size: medium;"><b>1. Introduction</b></span></span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">i) Language model: Pre-training has been shown to be effective for improving many NLP tasks</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">ii) 2 Strategies: feature-based (train only layers) or fine-tuning (update all including embeddings)</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">ELMO: uses task-specific architectures that include the pre-trained representations as additional features</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">GPT: is trained on the downstream tasks by simply fine-tuning all pre-trained parameters</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">Same objective function, and both unidirectional language models</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><strong><span style="font-family: helvetica;"><em>Unidirectionaity is the major limitation and very harmful in token-level tasks such as questions answering because it needs context informatio</em>n!</span></strong></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><strong><em><span style="font-family: helvetica;">Thus, a bidirectional model is needed - BERT!</span></em></strong></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">i) It alleviates unidirectionality constraint by using a Masked Language Model (MLM) pre-training objective</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">ii) MLM: randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary ID of the masked word based only on its context</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">iii) Next Sentence Prediction (NSP): joitly pre-trains text-pair representations</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><strong><span style="font-family: helvetica; font-size: medium;">Paper Contribution</span></strong></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">i) demonstrates the importance of bidirectional pre-training for language representations compared to GPT 1 (unidirectional) and ELMO (shallow concatenation of independently trained left-to-right and right-to-left LMs)</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">ii) the first fine-tuning based representation model outperforming many task-specific architectures.</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">iii) SOTA for 11 NLP tasks</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><strong><span style="font-family: helvetica;">2. Related work</span></strong></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">i) Unsupervised feature-based approaches: ELMO - generalizes traditional word embedding research along a different dimension; context-sensitive features from a left-to-right and a right-to-left language model</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">ii) Unsupervised fine-tuning approaches: OpenAI GPT - sentence or document encoders which produce contextual token representations have been pre-trained from unlabelled text</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">iii) Transfer learning from supervised data: Computer vision parts - effective transfer from supervised tasks with large datasets</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><strong><span style="font-family: helvetica;">3. BERT</span></strong></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">i) pre-training: training on unlabeled data over different pre-training tasks</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">ii) fine-tuning: first initializing with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks</span></p><figure class="wp-block-image size-large is-resized" style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;"><img alt="" class="wp-image-141" height="193" src="https://newlinguistcom.files.wordpress.com/2022/04/bert1.png?w=914" width="478" /></span></figure><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">iii) Model architecture: Transformers encoder</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">Bert base (L = 12, H = 768, A = 12, Total parameters = 110M) - compare this with GPT</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">Bert larger (L = 24, H = 1024, A = 16, Total parameters = 340 M)</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">Transformer (L = 6, H = 512, A = 8)</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">iii) Input/output representation</span></p><figure class="wp-block-image size-large" style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;"><img alt="" class="wp-image-143" height="146" src="https://newlinguistcom.files.wordpress.com/2022/04/bert2.png?w=509" width="467" /></span></figure><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">a) input - single sentence (span of contiguous text) or double sentences / WordPiece embeddings (30,000 vocabulary)</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">b) output: CLS - aggregate sequence representation for classification / SEP - sentence separation, segment embedding</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><strong><span style="font-family: helvetica;">Pre-training BERT</span></strong></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">Task 1: Masked LM</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">i) simply 15% input tokens mask at random, and then predict those masked tokens</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">ii) [MASK] token 80% + random token 10% + original 10% --&gt; avoid mismatch between pre-training and fine-tuning</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">Task 2: Next Sentence Prediction (NSP)</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">i) Many important downstream tasks such as Question Answering (QA) and Natural Language Inference (NLI) are based on understanding the relationship between two sentences</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">ii) 50% of the time B is the actual next sentence that follows A (labeled as IsNext), and 50% of the time it is a random sentence from the corpus (labeled as NotNext)</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">Pre-training data: BooksCorpus (800M words) + English Wikipedia (2,500M words).</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;"><strong>Fine-tuning BERT</strong>: simply plug in the task specific inputs and outputs into BERT and finetune all the parameters end-to-end</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">4. Experiments</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><strong><span style="font-family: helvetica;">GLUE (The General Language Understanding Evaluation benchmark)</span></strong></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">a) should perform well for many tasks, not just one</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">b) MNLI (Multi-Genre Natural Language Inference): entailment classification task</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">c) QQP (Quora Question Pairs): a task that checks whether question pairs are semantically similar to each other</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">d) QNLI(Question Natural Language Inference): a binary classification vertion of SQuAD. Whether paragraphs include answers or not.</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">e) SST-2(Stanford Sentiment Treebank): A binary classification task per sentence. Extracted from movie reviews with sentiment</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">f) CoLA(Corpus of Linguistic Acceptability): A binary classification task to check whether the English sentence is linguistically acceptable</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">g) STS-B (Semantic Textual Similarity Benchmark): How similar pairs of sentences are</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">h) MRPC(Microsoft Research Paraphrate Corpus): How similar pairs of sentences are</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">i) RTE(Recognizing Textual Entailment): similar to MNLI, but less data</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">j) WNLI(Winograd NLI): excluded in BERT models because of some evaluation-related issues</span></p><figure class="wp-block-image size-large" style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;"><img alt="" class="wp-image-152" height="113" src="https://newlinguistcom.files.wordpress.com/2022/04/bert3.png?w=629" width="487" /></span></figure><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><em><span style="text-decoration-line: underline;"><span style="font-family: helvetica;">BERT &gt; GPT, BERT LARGE &gt; BERT BASE</span></span></em></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><strong><span style="font-family: helvetica;">SQuAD v1.1: The Stanford Question Answering Dataset</span></strong></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">i) input question and passage as a single packed sequence, with the question using the A embedding and the passage using the B embedding</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">ii) training objective is the sum of the log-likelihoods of the correct start and end positions</span></p><figure class="wp-block-image size-large is-resized" style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;"><img alt="" class="wp-image-155" height="316" src="https://newlinguistcom.files.wordpress.com/2022/04/bert4.png?w=419" width="360" /></span></figure><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><strong><span style="font-family: helvetica;">SQuAD v2.0</span></strong></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">i) SQuAD v.1.1 + more than 50,000 unanswerable questions</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">ii) tasks are not just limited to when answers are possible, more difficult</span></p><figure class="wp-block-image size-large is-resized" style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;"><img alt="" class="wp-image-157" height="244" src="https://newlinguistcom.files.wordpress.com/2022/04/bert5.png?w=423" width="360" /></span></figure><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><strong><span style="font-family: helvetica;">SWAG (The Situations With Adversarial Generations dataset)</span></strong></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">i) evaluate grounded commonsense inference</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">ii) Given a sentence, the task is to choose the most plausible continuation among four choices</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">iii) four input sequences, each containing the concatenation of the given sentence (sentence<br />A) and a possible continuation (sentence B) ⇒ NSP</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><strong><span style="font-family: helvetica; font-size: medium;">5. Ablation Studies</span></strong></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><strong><span style="font-family: helvetica;">Effect of pre-training tasks</span></strong></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">i) No NSP ⇒ hurt performance QNLI, MNLI, and SQuAD 1.1</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">ii) LTR (rather than MLB) &amp; No NSP = same as GPT but larger training dataset, input representation, fine-tuning scheme</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">--&gt; LTR worse than MLM</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">--&gt; For SQuAD, LTR performs poorly</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">--&gt; BiLSTM, improve results on SQuAD, but worse in others</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">--&gt; possible to train separate LTR and RTL models and represent each token as the concatenation of the two models, as ELMo does BUT, 1) expensive, non-intuitive for tasks like QA, less powerful than a deep bidirectional model</span></p><figure class="wp-block-image size-large is-resized" style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;"><img alt="" class="wp-image-161" height="165" src="https://newlinguistcom.files.wordpress.com/2022/04/bert6.png?w=471" width="410" /></span></figure><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">Effect of model size: training a number of BERT models with a differing number of layers, hidden units, and attention heads, while otherwise using the same hyperparameters and training procedure</span></p><figure class="wp-block-image size-large is-resized" style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;"><img alt="" class="wp-image-163" height="204" src="https://newlinguistcom.files.wordpress.com/2022/04/bert7.png?w=410" width="388" /></span></figure><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">--&gt; larger models lead to a strict accuracy improvement across all four datasets</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">--&gt; scaling to extreme model sizes leads to large improvements on very small scale tasks, provided that the model has been sufficiently pre-trained (3,600 labeled training examples in MRPC)</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">Feature-based approach with BERT: the feature-based approach, where fixed features are extracted from the pre-trained model</span></p><figure class="wp-block-image size-large is-resized" style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;"><img alt="" class="wp-image-165" height="325" src="https://newlinguistcom.files.wordpress.com/2022/04/bert8.png?w=419" width="394" /></span></figure><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="text-decoration-line: underline;"><em><span style="font-family: helvetica;">Bert is effective for both fine-tuning and feature-based approaches!</span></em></span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><strong><span style="font-family: helvetica; font-size: medium;">Conclusion</span></strong></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">i) empirical improvements due to transfer learning with language models --&gt; rich, unsupervised pre-training is an integral part of many language understanding systems (low-resource tasks to benefit from deep unidirectional architectures)</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">ii) generalizing these findings to deep bidirectional architectures, allowing the same pre-trained model to successfully tackle a broad set of NLP tasks</span></p>

        <p class="post-meta">
            <strong>Categories:</strong> Paper Review, NLP
        </p>

        <p class="post-meta">
            <small>Original post: <a href="https://cheonkamjeong.blogspot.com/2022/12/paper-review-bert-pre-training-of-deep.html" target="_blank">https://cheonkamjeong.blogspot.com/2022/12/paper-review-bert-pre-training-of-deep.html</a></small>
        </p>
    </article>

    <!-- Navigation -->
    <nav>
        <ul>
            <li><a href="../index.html">Home</a></li>
            <li><a href="../publications.html">Publications</a></li>
            <li><a href="../blog.html" class="active">Blog</a></li>
        </ul>
    </nav>

    <!-- Footer -->
    <footer>
        <p>&copy; 2025 Cheonkam Jeong. Last updated: October 2025.</p>
    </footer>
</body>
</html>
