<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>[Paper Review - Speech Technology, NLP] WaveNet: A Generative Model for Raw Audio (van den Oord et al.,  2016) - Cheonkam Jeong</title>
    <link rel="stylesheet" href="../css/style.css">
</head>
<body>
    <article class="blog-post">
        <h1>[Paper Review - Speech Technology, NLP] WaveNet: A Generative Model for Raw Audio (van den Oord et al.,  2016)</h1>
        <p class="post-date">December 05, 2022</p>

        <p><span style="font-family: helvetica; font-size: medium;"><strong style="caret-color: rgb(0, 0, 0);">Abstract</strong></span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">WaveNet: a generative deep neural network that generates raw audio waveform</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">i) fully probabilistic (modeling using random variable and probability distribution)</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">ii) autoregressive (the predictive distribution for each audio sample conditioned on all previous ones)</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">text-to-speech, a single WaveNet can capture the characteristics of many different speakers with equal fidelity, music generation, phoneme recognition, etc.<span class="Apple-converted-space">&nbsp;</span></span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><strong><span style="font-family: helvetica; font-size: medium;">1. Introduction</span></strong></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">Modeling joint probabilities over pixels or words using neural architectures as products of conditional distributions yields SOTA generation.</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">WaveNet, an audio generative model based on the PixelCNN (van den Oord et al., 2016a;b)</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">Dilated causal convolution in order to deal with long-range temporal dependencies needed for raw audio generation</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;"><strong>Contributions:</strong><span class="Apple-converted-space">&nbsp;</span></span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">i) natural raw speech signal in TTS</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">ii) a single model can be used to generate different voices</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">iii) strong results when tested on a small speech recognition dataset and promising when used to generate other audio modalities such as music</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><strong><span style="font-family: helvetica; font-size: medium;">2. WaveNet</span></strong></p><figure class="wp-block-image size-large is-resized" style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;"><img alt="" class="wp-image-190" height="84" src="https://newlinguistcom.files.wordpress.com/2022/04/image-2.png?w=548" width="471" /></span></figure><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">i) each audio sample x_t is therefore conditioned on the samples at all previous timesteps</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">ii) a stack of convolutional layers</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">iii) no pooling layers, thus the time dimensionality of output and that of input is the same as each other</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">iv) output: categorical value via softmax</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">v) optimization: update parameters to maximize log likelihood</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><strong><span style="font-family: helvetica;">Dilated causal convolutions</span></strong></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><em><span style="font-family: helvetica;">Causal convolution</span></em></p><figure class="wp-block-image size-large is-resized" style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;"><img alt="" class="wp-image-192" height="181" src="https://newlinguistcom.files.wordpress.com/2022/04/image-3.png?w=474" width="435" /></span></figure><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">i) By using causal convolutions, we make sure the model cannot violate the ordering in which we model the data.</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">ii) the prediction emitted by the model at timestep t only depends on the previous timesteps</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">iii) training: parallel, generation: sequential</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">iv) no recurrent connection, thus fast</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">v) require many layers, or large filters to increase the receptive field --&gt; Dilated convolution can tackle this!</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><em><span style="font-family: helvetica;">Dilated convolution</span></em></p><figure class="wp-block-image size-large" style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;"><img alt="" class="wp-image-196" height="194" src="https://newlinguistcom.files.wordpress.com/2022/04/image-4.png?w=468" width="430" /></span></figure><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">i) A dilated convolution is a convolution where the filter is applied over an area larger than its length by skipping input values with a certain step</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">ii) large receptive fields with fewer layers</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">iii) input shape kept (less information loss)</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">iv) more discriminative because of more non-linear calculations</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><strong><span style="font-family: helvetica;">Softmax Distributions</span></strong></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">i) a softmax distribution tends to work better because a categorical distribution is more flexible and can more easily model arbitrary distributions because it makes no assumptions about their shape</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">ii) quantization: mu-law companding transformation to reduce the number of possible values</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">iii) nonlinear (mu-law compounding) &gt; linear (ReLu)</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;"><strong>Gated Activation Units:</strong><span class="Apple-converted-space">&nbsp;</span>the same gated activation unit as used in the gated PixelCNN</span></p><figure class="wp-block-image size-large" style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;"><img alt="" class="wp-image-199" height="63" src="https://newlinguistcom.files.wordpress.com/2022/04/image-5.png?w=540" width="442" /></span></figure><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">i) Element-wise multiplication of filter and gate</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">ii) Filter: dilated convolution and then tanh activation / local features from a certain layer</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">iii) Gate: dilated convolution and then sigmoid activation / decide how much the information of the filter will be passed to the next layer</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><strong><span style="font-family: helvetica;">Residual and Skip Connections</span></strong></p><figure class="wp-block-image size-large" style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;"><img alt="" class="wp-image-201" height="245" src="https://newlinguistcom.files.wordpress.com/2022/04/image-6.png?w=455" width="432" /></span></figure><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">i) to speed up convergence and enable training of much deeper models</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">ii) 1 * 1 convolution: less calculation, shaping</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;"><strong>Conditional</strong><span class="Apple-converted-space">&nbsp;</span><strong>WaveNets</strong></span></p><figure class="wp-block-image size-large is-resized" style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;"><img alt="" class="wp-image-203" height="52" src="https://newlinguistcom.files.wordpress.com/2022/04/image-7.png?w=240" width="240" /></span></figure><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">i) an addional input h to model the conditional distribution p(x|h) of the audio</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">ii) by conditioning the model on other input variables, we can guide WaveNet's generation to produce audio with the required characteristics</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">iii) conditioning the model on other inputs in 2 ways: global conditioning (a single latent representation h that influences the output distribution across all timesteps) and local conditioning (timeseries h_t; transform this series using a transposed convolutional network that maps it to a new time series y = f(h) with the same resolution as the audio signal, which is then used in the activation unit)</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;"><strong>Context Stacks:</strong><span class="Apple-converted-space">&nbsp;</span>another way to increase the receptive field</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">i) a complementary approach is to use a separate, smaller context stack that processes a long part of the audio signal and locally conditions a larger WaveNet that processes only a smaller part of the audio signal</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">ii) Shorter-range WaveNet is the main model + local conditioning</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><strong><span style="font-family: helvetica; font-size: medium;">3. Experiments</span></strong></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><strong><span style="font-family: helvetica;">Multi-speaker-speech generation</span></strong></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">i) Dataset: English multi-speeaker corpus from CSTR voice cloning toolkit</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">ii) The conditioning was applied by feeding the speaker ID to the model in the form of a one-hot vector</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">iii) Adding speakers resulted in better validation set performance compared to training solely on a single speaker</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">iii) The model picked up on other characteristics in the audio apart from the voice itself</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><strong><span style="font-family: helvetica;">Text-To-Speech</span></strong></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">i) Dataset: North American English and Mandarin Chinese dataset</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">ii) locally conditioned on linguistic features which were derived from input texts; the logarithmic fundamental frequency values in addition to the linguistic models</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">iii) WaveNet (both linguistic features + fundamental frequency) was the winner!</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><strong><span style="font-family: helvetica;">Music</span></strong></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">i) Dataset: MargnaTagATune dataset, YouTube piano dataset</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">ii) conditional music models, which can generate music given a set of tags specifying e.g. genre or instruments</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><strong><span style="font-family: helvetica;">Speech Recognition</span></strong></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">i) Dataset: TIMIT</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">ii) discrimination task</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">iii) For this task we added a mean-pooling layer after the dilated convolutions that aggregated the activations to coarser frames spanning 10 ms.</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">iv) best score</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><strong><span style="font-family: helvetica; font-size: medium;">4. Conclusion</span></strong></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">i) Deep generative model for audio data (waveform)</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">ii) autoregressive, dilated convolution</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">iii) can be conditioned on other in a global (e.g., speaker identity) or local way (e.g., linguistic features)</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">iv) promising results when applied to music audio modeling and speech recognition</span></p>

        <p class="post-meta">
            <strong>Categories:</strong> Paper Review, Speech Technology, NLP
        </p>

        <p class="post-meta">
            <small>Original post: <a href="https://cheonkamjeong.blogspot.com/2022/12/paper-review-wavenet-generative-model.html" target="_blank">https://cheonkamjeong.blogspot.com/2022/12/paper-review-wavenet-generative-model.html</a></small>
        </p>
    </article>

    <!-- Navigation -->
    <nav>
        <ul>
            <li><a href="../index.html">Home</a></li>
            <li><a href="../publications.html">Publications</a></li>
            <li><a href="../blog.html" class="active">Blog</a></li>
        </ul>
    </nav>

    <!-- Footer -->
    <footer>
        <p>&copy; 2025 Cheonkam Jeong. Last updated: October 2025.</p>
    </footer>
</body>
</html>
