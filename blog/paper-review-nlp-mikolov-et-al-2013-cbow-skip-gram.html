<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Mikolov et al., 2013 (CBOW & Skip Gram) - Cheonkam Jeong</title>
    <link rel="stylesheet" href="../css/style.css">
</head>
<body>
    <article class="blog-post">
        <h1>Mikolov et al., 2013 (CBOW & Skip Gram)</h1>
        <p class="post-date">March 31, 2022</p>

        <p>&nbsp;</p><h1><span style="font-family: helvetica; font-size: large;">Efficient estimation of word representations in vector space (Mikolov et al., 2013)</span></h1><h2><span style="font-family: helvetica; font-size: large;">Introduction</span></h2><h3 style="text-align: left;"><span style="font-family: helvetica; font-size: medium;">Traditional n-gram model</span></h3><ul><li><p><span style="font-family: helvetica;"><strong>Principle</strong>: prediction of next word based on previous n-1 words; probability of a sentence obtained by multiplying the probability (the n-gram model) of each word</span></p><p><span style="font-family: helvetica;">e.g., "This is a puppy"</span></p><p><span style="font-family: helvetica;">n = 1 (unigram): [This] [is] [a] [puppy] (this, is, a puppy)</span></p><p><span style="font-family: helvetica;">n = 2 (bigram): [This is] [is a] [a sentence] (this is, is a, a sentence)</span></p><p><span style="font-family: helvetica;">n = 3 (trigram): [This is a] [is a sentence] (this is a, is a sentence)</span></p></li><li><p><span style="font-family: helvetica;"><strong>Limits</strong>:</span></p><p><span style="font-family: helvetica;">i. insufficient amount of in-domain data (for ASR)</span></p><p><span style="font-family: helvetica;">ii. curse of dimensionality</span></p><p><span style="font-family: helvetica;">iii. difficulty of generalization (because the n-gram model has discrete space)</span></p><p><span style="font-family: helvetica;">iv. poor performance on word similarity tasks</span></p></li></ul><h3><span style="font-family: helvetica; font-size: medium;">Distributed representations of words as (continuous) vectors (vs. one-hot encoding)</span></h3><p><span style="font-family: helvetica;">e.g., queen â†’ | 0.313 | 0.123 | 0.326 | 0.128 | 0.610 | 0.415 | 0.120 |</span></p><p><span style="font-family: helvetica;">learning cost(?): the vocabulary size V * the number of parameters m (here 7)</span></p><ul><li><span style="font-family: helvetica;"><strong>Previous work on representation of words as continuous vectors</strong>: Feedforward NNLM</span></li><li><span style="font-family: helvetica;"><strong>Motivation and goal</strong>: necessity of training more complex models on much larger data set with a modest dimensionality of the word vectors between 50-100; building a model that shows better performance on word similarity tasks (multiple degrees of similarity)</span></li></ul><h2><span style="font-family: helvetica;">Model architectures</span></h2><h3 style="text-align: left;"><span style="font-family: helvetica; font-size: medium;">Some terms</span></h3><ul><li><p><span style="font-family: helvetica;"><strong>Computational complexity of a model:</strong>&nbsp;the number of parameters needed to fully train the model</span></p></li><li><p><span style="font-family: helvetica;"><strong>Training complexity</strong>&nbsp;is proportional to</span></p><p><span style="font-family: helvetica;">ðŸ’¡ O = E (number of training epochs; usually 3-50) * T (number of words in the training set; up to 1 billion) * Q (defined depending on model architecture)</span></p></li></ul><h3><span style="font-family: helvetica; font-size: medium;">Non-linear NN models</span></h3><ul><li><p><strong><span style="font-family: helvetica;">Feedforward NNLM</span></strong></p><p><span style="font-family: helvetica;">i. components: input, projection, hidden, and output layers</span></p><p><span style="font-family: helvetica;">ii. learning joint probability distribution of word sequences with word feature vectors using feedforward NN</span></p><p><span style="font-family: helvetica;">iii. input, output, and evaluation</span></p><p><span style="font-family: helvetica;">input: a concatenated feature vector of words</span></p><p><span style="font-family: helvetica;">output: the estimated probability of a sequence of n words</span></p><p><span style="font-family: helvetica;">a softmax function to evaluate the joint probability distribution of words</span></p><p><span style="font-family: helvetica;">iv. computational complexity per each training example</span></p><p><span style="font-family: helvetica;"><span>ðŸ’¡ Q = N (1-of-V coded input) * D +&nbsp;</span><strong>N * D * H</strong><span>&nbsp;(hidden layer size) + H * V (vocabulary size)</span></span></p></li></ul><p><em><span style="font-family: helvetica;">The number of output layers can be reduced to log2|V| (base 2) using hierarchical softmax layers</span></em></p><ul style="text-align: left;"><li><h3><strong><span style="font-family: helvetica;">Recurrent NNLM: RNN-based NNLM</span></strong></h3><p><span style="font-family: helvetica;">i. components: input (the current word vector &amp; hidden neuron values of the previous word), hidden, and output layers</span></p><p><span style="font-family: helvetica;">ii. predicting the current word</span></p><p><span style="font-family: helvetica;">iii. computational complexity per each training example</span></p><p><span style="font-family: helvetica;">&nbsp;ðŸ’¡ Q =&nbsp;<strong>H * H</strong>&nbsp;+ H * V</span></p><p><em><span style="font-family: helvetica;">The number of output layers can be reduced to log_2|V| using hierarchical softmax layers.</span></em></p><p><span style="font-family: helvetica;">For more on hierarchical softmax:</span></p><p><span style="font-family: helvetica;"><a href="http://building-babylon.net/2017/08/01/hierarchical-softmax/"></a><a href="http://building-babylon.net/2017/08/01/hierarchical-softmax/">http://building-babylon.net/2017/08/01/hierarchical-softmax/</a></span></p></li><li><h4><em><strong><span style="font-family: helvetica;"><br /></span></strong></em></h4></li><li><h3><strong><span style="font-family: helvetica;">New log-linear models</span></strong></h3><p><span style="font-family: helvetica;">i. The main observation from the previous section was that most of the complexity is caused by the non-linear hidden layer in the model</span></p><p><span style="font-family: helvetica;">ii. While this is what makes NNs so attractive, a simpler model that might not be able to represent the data as precisely as NNs but can be trained on much more data efficiently</span></p><p><span style="font-family: helvetica;">iii. The two architectures</span></p><p></p><div class="separator" style="clear: both; text-align: center;"><span style="font-family: helvetica;"><br /></span></div><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEin5o4yIeWMywYwzQOR1Q50N_Nsu4xOEjdFVV7UuS0GY6uN84HCRalSsRGnpuYhTkJ9H3HkAUuqySyRUHVCCUn3Ivdjc45Vy5B8ZQC3WDCkKVnAv-m_Me_cOjoql0SSbV0la3x0k0ExWszGqNEUDrG0wX8v_fwe6kcJb_akW4XuUWPWnfH4iM2kizha" style="clear: left; float: left; margin-bottom: 1em; margin-left: 1em;"><span style="font-family: helvetica;"><img alt="" data-original-height="1024" data-original-width="1458" height="303" src="https://blogger.googleusercontent.com/img/a/AVvXsEin5o4yIeWMywYwzQOR1Q50N_Nsu4xOEjdFVV7UuS0GY6uN84HCRalSsRGnpuYhTkJ9H3HkAUuqySyRUHVCCUn3Ivdjc45Vy5B8ZQC3WDCkKVnAv-m_Me_cOjoql0SSbV0la3x0k0ExWszGqNEUDrG0wX8v_fwe6kcJb_akW4XuUWPWnfH4iM2kizha=w431-h303" width="431" /></span></a></div><span style="font-family: helvetica;"><br /><br /><br /></span><p></p><p><em><strong><span style="font-family: helvetica;"><br /></span></strong></em></p><p><em><strong><span style="font-family: helvetica;"><br /></span></strong></em></p><p><em><strong><span style="font-family: helvetica;"><br /></span></strong></em></p><p><em><strong><span style="font-family: helvetica;"><br /></span></strong></em></p><p><em><strong><span style="font-family: helvetica;"><br /></span></strong></em></p><p><em><strong><span style="font-family: helvetica;"><br /></span></strong></em></p><p><em><strong><span style="font-family: helvetica;"><br /></span></strong></em></p><p><span style="font-family: helvetica;"><br /></span></p><h4><em><strong><span style="font-family: helvetica;">Continuous Bag-of-words Model</span></strong></em></h4><p><span style="font-family: helvetica;">Similar to feedforward NNLM without the hidden layer</span></p><p><span style="font-family: helvetica;">Q = N * D + D * log_2(V)</span></p><h4><em><strong><span style="font-family: helvetica;">Continuous Skip-gram Model</span></strong></em></h4><p><span style="font-family: helvetica;">to predict words adjacent to the current word</span></p><p><span style="font-family: helvetica;">Q = C (max distance of the words) * (D + D * log_2(V))</span></p><p><span style="font-family: helvetica;">if C = 5, a number R in range &lt; 1;C &gt; is selected randomly, and then R words from past and those from future of the current word as correct labels. So, R * 2 word classifications with the current word as input and each of the R + R words as output.</span></p></li></ul><h2><span style="font-family: helvetica;">Results</span></h2><h3 style="text-align: left;"><strong><span style="font-family: helvetica;"><span>&nbsp;&nbsp; &nbsp;<span>&nbsp;&nbsp; &nbsp;</span></span>Task description</span></strong></h3><div style="text-align: left;"><span style="font-family: helvetica;"><span>&nbsp;&nbsp; &nbsp;<span>&nbsp;&nbsp; &nbsp;<span>&nbsp;&nbsp; &nbsp;</span></span></span>i. a comprehensive test set that contains 5 types of semantic questions and 9 types of</span></div><div style="text-align: left;"><span style="font-family: helvetica;"><span>&nbsp;&nbsp; &nbsp;<span>&nbsp;&nbsp; &nbsp;<span>&nbsp;&nbsp; &nbsp;</span></span></span>syntactic questions (overall, 8869 semantic and 10675 syntactic questions)</span></div><div style="text-align: left;"><blockquote style="border: none; margin: 0px 0px 0px 40px; padding: 0px;"><div style="text-align: left;"><img alt="" data-original-height="844" data-original-width="1332" height="284" src="https://blogger.googleusercontent.com/img/a/AVvXsEhBNnlB5AlgB4wBlCciuWSOmvB2SGvzfLWcxOvbZu8EJcG6cKFCS6POK5Eq5eGQOq13DHQzo2YzsqcOerDVatK7m-7O0iurxlneEYoW0vEKExHAfdwZSxU4L_tfTKFI3grsLsv3AJQ3HbHui-wNs7ra2n2MLhKW0rGCypnVI6yTKhYDLzXVytZPOlzt=w450-h284" style="font-family: helvetica;" width="450" /></div></blockquote></div><ul style="text-align: left;"><li><p><span style="font-family: helvetica;">ii. evaluation: accuracy for each question type separately</span></p><h3><strong><span style="font-family: helvetica;">Maximization of accuracy</span></strong></h3><h4><em><span style="font-family: helvetica;">Data: a Google News corpus for training the word vectors (6B tokens)</span></em></h4><div><span style="font-family: helvetica;">i. restricted the vocabulary size to 1 million most frequent words</span></div></li><li><p><span style="font-family: helvetica;">ii. first evaluated models trained on subsets of the training data (the most frequent 30k words)</span></p><p><span style="font-family: helvetica;"></span></p><div class="separator" style="clear: both; text-align: center;"><span style="font-family: helvetica;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEgVhfnpxXMi8oBstBLau7YUSyv0M8ZBaFvcAgFmoPpxPaFEmRcArAQsZZtTGXqEnaIn_9eLj5AZFjcr9zGakfK-_uGPacolBgJQ0CunTRSUxzz5iM72PsCazqgqrOZr-uQiMPPqFzgmIkfWQTR_FPWzX9308yFxyCNieVAH_17uRbmnirqd0lOovTil" style="clear: left; float: left; margin-bottom: 1em; margin-right: 1em;"><img alt="" data-original-height="414" data-original-width="1300" height="144" src="https://blogger.googleusercontent.com/img/a/AVvXsEgVhfnpxXMi8oBstBLau7YUSyv0M8ZBaFvcAgFmoPpxPaFEmRcArAQsZZtTGXqEnaIn_9eLj5AZFjcr9zGakfK-_uGPacolBgJQ0CunTRSUxzz5iM72PsCazqgqrOZr-uQiMPPqFzgmIkfWQTR_FPWzX9308yFxyCNieVAH_17uRbmnirqd0lOovTil=w451-h144" width="451" /></a></span></div><span style="font-family: helvetica;"><br /><br /></span><p></p><p><span style="font-family: helvetica;"><br /></span></p><p><span style="font-family: helvetica;"><br /></span></p><p><span style="font-family: helvetica;"><br /></span></p><p><span style="font-family: helvetica;"><br /></span></p><p><span style="font-family: helvetica;">iii. 3 training epochs with SGD and backpropagation; learning rate decreases from 0.025</span></p><h3><strong><span style="font-family: helvetica;">Comparison of model architectures</span></strong></h3><h4><em><span style="font-family: helvetica;">Data: several LDC corpora (320M words, 82k vocabulary)</span></em></h4></li><li><p><em><span style="font-family: helvetica;"></span></em></p><div class="separator" style="clear: both; text-align: center;"><em><span style="font-family: helvetica;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEhSJVCPjCPIfQE4sEpyZPznxGUhCIiYZio2tPwhCyaeRNm1W53UhPmVWgwSU9vH2rg150kuMcDVUM4ZnSncgnVoaRAAnpXU0HhCL5o2DreYOLaSrtkXPEpNKLcq3vGXcPgheAf4ZWUhtXjpXiO7Np9g0perDoP0PghgX71q46uMM0-WIqLyRMamRhUG" style="clear: left; float: left; margin-bottom: 1em; margin-right: 1em;"><img alt="" data-original-height="458" data-original-width="1332" height="157" src="https://blogger.googleusercontent.com/img/a/AVvXsEhSJVCPjCPIfQE4sEpyZPznxGUhCIiYZio2tPwhCyaeRNm1W53UhPmVWgwSU9vH2rg150kuMcDVUM4ZnSncgnVoaRAAnpXU0HhCL5o2DreYOLaSrtkXPEpNKLcq3vGXcPgheAf4ZWUhtXjpXiO7Np9g0perDoP0PghgX71q46uMM0-WIqLyRMamRhUG=w456-h157" width="456" /></a></span></em></div><em><span style="font-family: helvetica;"><br /><br /></span></em><p></p><p><em><span style="font-family: helvetica;"></span></em></p><div class="separator" style="clear: both; text-align: center;"><em><span style="font-family: helvetica;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEheZpkw1mBjAQ4DmFEoyA9lHVv3X8Fo3iO1LKUXxMv-INw_LF97mCn50N7aCQ5i0xbpE0zDX_QYFDnBc6s3mq3eTUlpvhv3DrUH-FFHMnH4IJ0lbK4O6z1FA81jOuTX1UsvS8uB5zPKKsv0ZtjEH1RKgUf4DJ6YryzJKnxRa7goLMO9ZXSwzIiwukEQ" style="clear: left; float: left; margin-bottom: 1em; margin-right: 1em;"><img alt="" data-original-height="1514" data-original-width="1374" height="499" src="https://blogger.googleusercontent.com/img/a/AVvXsEheZpkw1mBjAQ4DmFEoyA9lHVv3X8Fo3iO1LKUXxMv-INw_LF97mCn50N7aCQ5i0xbpE0zDX_QYFDnBc6s3mq3eTUlpvhv3DrUH-FFHMnH4IJ0lbK4O6z1FA81jOuTX1UsvS8uB5zPKKsv0ZtjEH1RKgUf4DJ6YryzJKnxRa7goLMO9ZXSwzIiwukEQ=w453-h499" width="453" /></a></span></em></div><em><span style="font-family: helvetica;"><br /><br /></span></em><p></p><p><em><span style="font-family: helvetica;"><br /></span></em></p><p><em><span style="font-family: helvetica;"><br /></span></em></p><p><em><span style="font-family: helvetica;"><br /></span></em></p><p><em><span style="font-family: helvetica;"><br /></span></em></p><p><em><span style="font-family: helvetica;"><br /></span></em></p><p><em><span style="font-family: helvetica;"><br /></span></em></p><p><em><span style="font-family: helvetica;"><br /></span></em></p><p><em><span style="font-family: helvetica;"><br /></span></em></p><p><em><span style="font-family: helvetica;"><br /></span></em></p><p><em><span style="font-family: helvetica;"><br /></span></em></p><p><em><span style="font-family: helvetica;"><br /></span></em></p><p><em><span style="font-family: helvetica;"><br /></span></em></p><p><em><span style="font-family: helvetica;"><br /></span></em></p><p><br /></p><h3><strong><span style="font-family: helvetica;">Large scale parallel training of model&nbsp;</span></strong></h3><p><span style="font-family: helvetica;">Several models trained on the Google News 6B data set, with mini-batch asynchronous gradient descent and the adaptive learning rate procedure (Adagrad)</span></p></li></ul><ul style="text-align: left;"><span style="font-family: helvetica;"><img alt="" data-original-height="898" data-original-width="1342" height="307" src="https://blogger.googleusercontent.com/img/a/AVvXsEhcw1wMMFnI7cDGedHx_SPq8Xwe0-u6ENiCkr-Qn7t3xdP-pEY0GIQXGt9M3UXe4nxrdqRyNpmNIdGv4I1dXP0rxaaLul7lSPTPPAdokFXzWNCIwmZ25dh6Hf-qboqroXdRqd8ynhYNmcC07TJ0itl8_WTMwCKC4Ttpn0VUTXwvx3QE4sY4jiEPolX-=w458-h307" width="458" /></span><span style="font-family: helvetica;"><div><span style="font-size: medium;"><b>Examples of the learned relationships</b></span></div></span><span style="font-family: helvetica;"><div><a href="#" style="text-size-adjust: auto;"><img alt="" height="225" src="https://blogger.googleusercontent.com/img/a/AVvXsEjWZ9FEAO3PaKRNCrsV9ob1Y_hoPXFX9mQYYn6jilg60pSTKvSVC0oBSyeDdAs6uhs4IEAHsFli_m69Q9HgFePQTxdGeEOPNecHTGMqJSW0ybtAsB_ZFh6RRed_hn_-16Ha4znMysvzMu4gpDQCACyOXKkhg_WCMKqffrSHpEs24LGmdCxN3S6UpOsz=w451-h225" width="451" /></a></div></span><span style="font-family: helvetica;"><div><b><span style="font-size: large;">Conclusion</span></b></div></span><span style="font-family: helvetica;"><div><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;">It is possible to train high quality word vectors using very simple model architectures</p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;">It is possible to compute very accurate high dimensional word vectors from a much larger data set</p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;">The word vectors can be successfully applied to automatic extension of facts in Knowledge Bases, and also for verification of correctness of existing facts</p></div></span><span style="font-family: helvetica;"><br /></span></ul>

        <p class="post-meta">
            <strong>Categories:</strong> Paper Review, NLP
        </p>

        <p class="post-meta">
            <small>Original post: <a href="https://cheonkamjeong.blogspot.com/2022/03/paper-review-mikolov-et-al-2013-cbow.html" target="_blank">https://cheonkamjeong.blogspot.com/2022/03/paper-review-mikolov-et-al-2013-cbow.html</a></small>
        </p>
    </article>

    <!-- Navigation -->
    <nav>
        <ul>
            <li><a href="../index.html">Home</a></li>
            <li><a href="../publications.html">Publications</a></li>
            <li><a href="../blog.html" class="active">Blog</a></li>
        </ul>
    </nav>

    <!-- Footer -->
    <footer>
        <p>&copy; 2025 Cheonkam Jeong. Last updated: October 2025.</p>
    </footer>
</body>
</html>
