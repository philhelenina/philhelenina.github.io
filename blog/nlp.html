<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Nlp - Cheonkam Jeong</title>
    <link rel="stylesheet" href="../css/style.css">
</head>
<body>
    <h1>Nlp</h1>

    <p><a href="../blog.html">‚Üê Back to all posts</a></p>

    <div class="blog-post">
        <h2><a href="speech-technology-nlp-differences-between-hidden-markov-models-perceptron-and-full-neural-networks.html">[Speech Technology, NLP] Differences Between Hidden Markov Models, Perceptron, and Full Neural Networks</a></h2>
        <p class="post-date">January 04, 2023</p>
        <p>
            Both HMMs and neural nets, including perceptron, have in common that they basically identify whether an item is a member of the class. However, they are essentially different from each other; HMMs are...
        </p>
    </div>

    <div class="blog-post">
        <h2><a href="nlp-data-a-list-of-korean-acoustic-corpora.html">[NLP - Data] A list of Korean Acoustic Corpora</a></h2>
        <p class="post-date">December 21, 2022</p>
        <p>
            The Speech Corpus of Reading-Style Standard Korean (NIKL 2005;&nbsp;https://github.com/homink/speech.ko)120 hours (??)Read speech&nbsp;120 speakers -- gender balanced (60 males; 60 females) and the ag...
        </p>
    </div>

    <div class="blog-post">
        <h2><a href="python-code-nlp-how-to-install-konlpy-a-korean-nlp-library-on-your-mac-using-anaconda-.html">[Python Code, NLP] How to install KoNLPy (a Korean NLP library) on your Mac using Anaconda? </a></h2>
        <p class="post-date">December 21, 2022</p>
        <p>
            Follow the instruction on the official page:&nbsp;https://konlpy.org/ko/latest/install/#If you are using Ubuntu or Window, there might not be a problem (When I installed KoNLPy on my Window machine, I...
        </p>
    </div>

    <div class="blog-post">
        <h2><a href="book-summary-nlp-deep-learning-chs6-10-ian-goodfellow-2016.html">[Book Summary, NLP] Deep Learning Chs.6-10 (Ian Goodfellow, 2016)</a></h2>
        <p class="post-date">December 14, 2022</p>
        <p>
            To download summary files, click the link below:&nbsp;https://drive.google.com/drive/folders/1Nj96nLM73ELrz6gw30Da4CIT6Sunu70n?usp=sharing
        </p>
    </div>

    <div class="blog-post">
        <h2><a href="paper-review-in-korean-nlp-enriching-word-vectors-with-subword-information-aka-fast-text-bojanowski-.html">[Paper Review in Korean - NLP] Enriching Word Vectors with Subword Information (a.k.a. Fast Text) - Bojanowski et al. 2016</a></h2>
        <p class="post-date">December 05, 2022</p>
        <p>
            Presentation Slides by Cheonkam Jeong&nbsp;
        </p>
    </div>

    <div class="blog-post">
        <h2><a href="paper-review-nlp-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-dev.html">[Paper Review - NLP] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding  (Devlin et al., 2021)</a></h2>
        <p class="post-date">December 05, 2022</p>
        <p>
            &nbsp;AbstractIntroducing BERT (Bidirectional Encoder Representations from Transformers); bidirectional - jointly conditioning on both left and right context in all layers, which creates state-of-art...
        </p>
    </div>

    <div class="blog-post">
        <h2><a href="paper-review-speech-technology-nlp-wavenet-a-generative-model-for-raw-audio-van-den-oord-et-al-2016.html">[Paper Review - Speech Technology, NLP] WaveNet: A Generative Model for Raw Audio (van den Oord et al.,  2016)</a></h2>
        <p class="post-date">December 05, 2022</p>
        <p>
            AbstractWaveNet: a generative deep neural network that generates raw audio waveformi) fully probabilistic (modeling using random variable and probability distribution)ii) autoregressive (the predictiv...
        </p>
    </div>

    <div class="blog-post">
        <h2><a href="paper-review-nlp-negative-sampling-mikolov-et-al-2013.html">[Paper Review - NLP] Negative Sampling (Mikolov et al., 2013)</a></h2>
        <p class="post-date">December 04, 2022</p>
        <p>
            &nbsp;Goalsimprove skip-gram model in terms of speed and accuracymake it possible to make word representation not limited to individual words; rather, it aims to make it possible to represent idiomati...
        </p>
    </div>

    <div class="blog-post">
        <h2><a href="paper-review-nlp-mikolov-et-al-2013-cbow-skip-gram.html">[Paper Review - NLP] Mikolov et al., 2013 (CBOW & Skip Gram)</a></h2>
        <p class="post-date">March 31, 2022</p>
        <p>
            &nbsp;Efficient estimation of word representations in vector space (Mikolov et al., 2013)IntroductionTraditional n-gram modelPrinciple: prediction of next word based on previous n-1 words; probability...
        </p>
    </div>

    <!-- Navigation -->
    <nav>
        <ul>
            <li><a href="../index.html">Home</a></li>
            <li><a href="../publications.html">Publications</a></li>
            <li><a href="../blog.html" class="active">Blog</a></li>
        </ul>
    </nav>

    <!-- Footer -->
    <footer>
        <p>&copy; 2025 Cheonkam Jeong. Last updated: October 2025.</p>
    </footer>
</body>
</html>
