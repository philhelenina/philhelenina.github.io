<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>[Paper Review - NLP] Negative Sampling (Mikolov et al., 2013) - Cheonkam Jeong</title>
    <link rel="stylesheet" href="../css/style.css">
</head>
<body>
    <article class="blog-post">
        <h1>[Paper Review - NLP] Negative Sampling (Mikolov et al., 2013)</h1>
        <p class="post-date">December 04, 2022</p>

        <p style="text-align: left;">&nbsp;<strong style="caret-color: rgb(0, 0, 0);"><span style="font-family: helvetica; font-size: large;">Goals</span></strong></p><ul style="caret-color: rgb(0, 0, 0); text-align: left; text-size-adjust: auto;"><li><span style="font-family: helvetica;">improve skip-gram model in terms of speed and accuracy</span></li><li><span style="font-family: helvetica;">make it possible to make word representation not limited to individual words; rather, it aims to make it possible to represent idiomatic expressions (e.g., Boston Globe)</span></li></ul><p style="caret-color: rgb(0, 0, 0); text-align: left; text-size-adjust: auto;"><strong><span style="font-family: helvetica; font-size: large;">The original skip-gram model</span></strong></p><ul style="caret-color: rgb(0, 0, 0); text-align: left; text-size-adjust: auto;"><li><span style="font-family: helvetica;">Skip-gram model: aims to predict the words that surround the current words; thus, aims to high log probability</span></li></ul><figure class="wp-block-image size-large is-resized" style="caret-color: rgb(0, 0, 0); text-align: left; text-size-adjust: auto;"><span style="font-family: helvetica;"><img alt="" class="wp-image-82" height="99" src="https://newlinguistcom.files.wordpress.com/2022/04/skip-gram-fm.png?w=568" width="268" /></span><figcaption class="wp-element-caption"><span style="font-family: helvetica;"><em>c: the size of the training context (can be a function of the center word w_t)</em><span class="Apple-converted-space">&nbsp;</span>/ p(w_t+j|w_t): determined by softmax</span></figcaption></figure><figure class="wp-block-image size-large is-resized" style="caret-color: rgb(0, 0, 0); text-align: left; text-size-adjust: auto;"><span style="font-family: helvetica;"><img alt="" class="wp-image-84" height="108" src="https://newlinguistcom.files.wordpress.com/2022/04/skip-gram-fm2.png?w=532" width="298" /></span><figcaption class="wp-element-caption"><span style="font-family: helvetica;">v_w: input, v'_w: output vector representations of, w: the number of words in the vocabulary<span class="Apple-converted-space">&nbsp;</span></span></figcaption></figure><ul style="caret-color: rgb(0, 0, 0); text-align: left; text-size-adjust: auto;"><li><span style="font-family: helvetica;">Hierarchical Softmax(HS): replacing softmax to reduce time cost due to enormous amount of calculations</span></li></ul><figure class="wp-block-image size-large is-resized" style="caret-color: rgb(0, 0, 0); text-align: left; text-size-adjust: auto;"><span style="font-family: helvetica;"><img alt="" class="wp-image-86" height="273" src="https://newlinguistcom.files.wordpress.com/2022/04/hs.png?w=1024" width="495" /></span></figure><figure class="wp-block-image size-large" style="caret-color: rgb(0, 0, 0); text-align: left; text-size-adjust: auto;"><span style="font-family: helvetica;"><img alt="" class="wp-image-88" height="517" src="https://newlinguistcom.files.wordpress.com/2022/04/huffman.png?w=984" width="497" /></span></figure><p style="caret-color: rgb(0, 0, 0); text-align: left; text-size-adjust: auto;"><strong><span style="font-family: helvetica; font-size: large;">Methods for improvement</span></strong></p><ul style="caret-color: rgb(0, 0, 0); text-align: left; text-size-adjust: auto;"><li><span style="font-family: helvetica;">Negative samplings (NEG): let's reduce unnecessary calculations by not updating irrelevant words</span></li></ul><blockquote style="border: none; margin: 0px 0px 0px 40px; padding: 0px; text-align: left;"><p style="caret-color: rgb(0, 0, 0); text-align: left; text-size-adjust: auto;"><span style="font-family: helvetica;">i) Noise Contrastive Estimation (NCE): a good model should separate data from noise using logistic regression.</span></p><p style="caret-color: rgb(0, 0, 0); text-align: left; text-size-adjust: auto;"><span style="font-family: helvetica;">ii) a simplified version of NCE, which both requires numerical probabilities of the noise distribution and samples, while NEG requires samples only</span></p><p style="caret-color: rgb(0, 0, 0); text-align: left; text-size-adjust: auto;"><span style="font-family: helvetica;">iii) NCE aims to improve log probabilities of softmax (accuracy). The skip-gram model aims to improve the quality of vector representation. NEG is also considered in this regard.</span></p></blockquote><figure class="wp-block-image size-large is-resized" style="caret-color: rgb(0, 0, 0); text-align: left; text-size-adjust: auto;"><span style="font-family: helvetica;"><img alt="" class="wp-image-92" height="76" src="https://newlinguistcom.files.wordpress.com/2022/04/neg.png?w=872" width="429" /></span><figcaption class="wp-element-caption"><span style="font-family: helvetica;">if the first term gets bigger, it closes to 0;, the calculation for wrong pairs (-v'_wi^tV_wI) will show the opposite</span></figcaption></figure><ul style="caret-color: rgb(0, 0, 0); text-align: left; text-size-adjust: auto;"><li><span style="font-family: helvetica;">Subsampling of frequent words: frequent words tend not to be informative; thus, to solve this problem, this method is proposed</span></li></ul><figure class="wp-block-image size-large is-resized" style="caret-color: rgb(0, 0, 0); text-align: left; text-size-adjust: auto;"><span style="font-family: helvetica;"><img alt="" class="wp-image-99" height="103" src="https://newlinguistcom.files.wordpress.com/2022/04/subsampling.png?w=392" width="208" /></span><figcaption class="wp-element-caption"><span style="font-family: helvetica;">P(wi): the probability of discarding the word; f(wi): the frequency of the world; t(threshold)</span></figcaption></figure><blockquote style="border: none; margin: 0px 0px 0px 40px; padding: 0px;"><p style="caret-color: rgb(0, 0, 0); text-align: left; text-size-adjust: auto;"><span style="font-family: helvetica;">The more frequent a word is, the more likely it is to be discarded =&gt; This solves some problems caused by imbalance of frequency; This also increases the training speed and vector accuracy.</span></p></blockquote><ul style="caret-color: rgb(0, 0, 0); text-align: left; text-size-adjust: auto;"><li><span style="font-family: helvetica;">Experiments (analogical reasoning task for phrases)</span></li></ul><blockquote style="border: none; margin: 0px 0px 0px 40px; padding: 0px; text-align: left;"><p style="caret-color: rgb(0, 0, 0); text-align: left; text-size-adjust: auto;"><span style="font-family: helvetica;">i) Task: what is 4th phrase based on the previous three phrases? (e.g., Germany: Berlin: France: ?), including syntactic and semantic analogies</span></p><p style="caret-color: rgb(0, 0, 0); text-align: left; text-size-adjust: auto;"><span style="font-family: helvetica;">ii) Data: an internal Google dataset with one billion words (training data), a total of 692k after taking out words whose frequency is less than 5</span></p><p style="caret-color: rgb(0, 0, 0); text-align: left; text-size-adjust: auto;"><span style="font-family: helvetica;">iii) Result: (Accuracy) NEG &gt; HS, NEG &gt; NCE, (Speed) Subsampling &gt; the others / The linear property of the skip-gram model contributes this analogical test, but if a much bigger training dataset is provided, the skip-gram model can be successful in non-linear models</span></p></blockquote><ul style="caret-color: rgb(0, 0, 0); text-align: left; text-size-adjust: auto;"><li><span style="font-family: helvetica;">Word-based =&gt; Phrase-based using a data-driven approach</span></li><li><span style="font-family: helvetica;">Experiments (Analogical reasoning task for phrases)</span></li></ul><blockquote style="border: none; margin: 0px 0px 0px 40px; padding: 0px; text-align: left;"><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">i) Task: New analogical task</span></p></blockquote><blockquote style="border: none; margin: 0px 0px 0px 40px; padding: 0px; text-align: left;"><img alt="" class="wp-image-95" height="281" src="https://newlinguistcom.files.wordpress.com/2022/04/analg-task.png?w=1024" style="caret-color: rgb(0, 0, 0); font-family: helvetica;" width="496" /></blockquote><blockquote style="border: none; margin: 0px 0px 0px 40px; padding: 0px; text-align: left;"><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">ii) Results: NEG-15 (k=15) &gt; NEG-5 (k=5), Subsampling models &gt; No subsampling ones, Subsampling HS-Huffman &gt; No subsampling one</span></p><p style="caret-color: rgb(0, 0, 0); text-size-adjust: auto;"><span style="font-family: helvetica;">iii) With 33 billion words, HS, dim = 1,000, c = entire sentence, 72% accuracy; A combination of HS &amp; Subsampling showed the best performance</span></p></blockquote><p style="caret-color: rgb(0, 0, 0); text-align: left; text-size-adjust: auto;"><strong><span style="font-family: helvetica; font-size: large;">Additive compositionally</span></strong></p><p style="caret-color: rgb(0, 0, 0); text-align: left; text-size-adjust: auto;"><span style="font-family: helvetica;">The skip-gram model can make meaningful words using element-wise addition</span></p><figure class="wp-block-image size-large is-resized" style="caret-color: rgb(0, 0, 0); text-align: left; text-size-adjust: auto;"><span style="font-family: helvetica;"><img alt="" class="wp-image-97" height="130" src="https://newlinguistcom.files.wordpress.com/2022/04/additive.png?w=1024" width="498" /></span></figure>

        <p class="post-meta">
            <strong>Categories:</strong> Paper Review, NLP
        </p>

        <p class="post-meta">
            <small>Original post: <a href="https://cheonkamjeong.blogspot.com/2022/12/paper-review-negative-sampling-mikolov.html" target="_blank">https://cheonkamjeong.blogspot.com/2022/12/paper-review-negative-sampling-mikolov.html</a></small>
        </p>
    </article>

    <!-- Navigation -->
    <nav>
        <ul>
            <li><a href="../index.html">Home</a></li>
            <li><a href="../publications.html">Publications</a></li>
            <li><a href="../blog.html" class="active">Blog</a></li>
        </ul>
    </nav>

    <!-- Footer -->
    <footer>
        <p>&copy; 2025 Cheonkam Jeong. Last updated: October 2025.</p>
    </footer>
</body>
</html>
